[{"title": "Counterintuitive Properties of High Dimensional Space", "date": "2018-03-03", "description": "Our geometric intuition developed in our three-dimensional world often fails us in higher dimensions. In this post I describe just a few phenomena of high dimensions, including escaping spheres, concentration of measure, and kissing numbers. All of these properties can be illustrated using basic mathematics.", "slug": "counterintuitive-properties-of-high-dimensional-space", "tags": ["geometry", "high dimensions"], "authors": [{"name": "Marc Khoury"}], "comments": true, "html": "<p>Our geometric intuition developed in our three-dimensional world often fails us in higher dimensions. Many properties of even simple objects, such as higher dimensional analogs of cubes and spheres, are very counterintuitive. Below we discuss just a few of these properties in an attempt to convey some of the weirdness of high dimensional space.</p>\n<p>You may be used to using the word \u201ccircle\u201d in two dimensions and \u201csphere\u201d in three dimensions. However, in higher dimensions we generally just use the word sphere, or \\(d\\)-sphere when the dimension of the sphere is not clear from context. With this terminology, a circle is also called a 1-sphere, for a 1-dimensional sphere. A standard sphere in three dimensions is called a 2-sphere, and so on. This sometimes causes confusion, because a \\(d\\)-sphere is usually thought of as existing in \\((d+1)\\)-dimensional space. When we say \\(d\\)-sphere, the value of \\(d\\) refers to the dimension of the sphere locally on the object, not the dimension in which it lives. Similarly we\u2019ll often use the word cube for a square, a standard cube, and its higher dimensional analogues.</p>\n<h2 id=\"escaping-spheres\">Escaping Spheres</h2>\n<p>Consider a square with side length 1. At each corner of the square place a circle of radius 1/2, so that the circles cover the edges of the square. Then consider the circle centered at the center of the square that is just large enough to touch the circles at the corners of the square. In two dimensions it\u2019s clear that the inner circle is entirely contained in the square.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure1.png\" width=\"300\">\n<figcaption>\n<b>Figure 1:</b> At each corner of the square we place a circle of radius 1/2. The inner circle is just large enough to touch the circles at the corners.<br />\n\n</figcaption>\n</figure>\n<p>We can do the same thing in three dimensions. At each corner of the unit cube place a sphere of radius 1/2, again covering the edges of the cube. The sphere centered at the center of the cube and tangent to spheres at the corners of the cube is shown in red in Figure 2. Again we see that, in three dimensions, the inner sphere is entirely contained in the cube.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure2.png\">\n<figcaption>\n<b>Figure 2:</b> In three dimensions we place a sphere at the each of the eight corners of a cube.\n</figcaption>\n</figure>\n<p>To understand what happens in higher dimensions we need to compute the radius of the inner sphere in terms of the dimension. The radius of the inner sphere is equal to the length of the diagonal of the cube minus the radius of the spheres at the corners. See Figure 3. The latter value is always 1/2, regardless of the dimension. We can compute the length of the diagonal as</p>\n<span class=\"math display\">\\[\\begin{aligned}\nd((\\frac{1}{2}, \\frac{1}{2}, \\ldots, \\frac{1}{2}), (1,1, \\ldots, 1)) &amp;= \\sqrt{\\sum_{i = 1}^{d} (1 - 1/2)^2}\\\\\n&amp;= \\sqrt{d}/2\n\\end{aligned}\\]</span>\n<p>Thus the radius of the inner sphere is \\(\\sqrt{d}/2 - 1/2\\). Notice that the radius of the inner sphere is increasing with the dimension!</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure3.png\">\n<figcaption>\n<b>Figure 3:</b> The size of the radius of the inner sphere is growing as the dimension increases because the distance to the corner increases while the radius of the corner sphere remains constant.\n</figcaption>\n</figure>\n<p>In dimensions two and three, the sphere is strictly inside the cube, as we\u2019ve seen in the figures above. However in four dimensions something very interesting happens. The radius of the inner sphere is exactly 1/2, which is just large enough for the inner sphere to touch the sides of the cube! In five dimensions, the radius of the inner sphere is 0.618034, and the sphere starts poking outside of the cube! By ten dimensions, the radius is 1.08114 and the sphere is poking very far outside of the cube!</p>\n<h2 id=\"volume-in-high-dimensions\">Volume in High Dimensions</h2>\n<p>The area of a circle \\( A(r) = r^2\\), where \\(r\\) is the radius. Given the equation for the area of a circle, we can compute the volume of a sphere by considering cross sections of the sphere. That is, we intersect the sphere with a plane at some height \\( h \\) above the center of the sphere.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure4.png\" width=\"300\">\n<figcaption>\n<b>Figure 4:</b> Intersecting the sphere with a plane gives a circle.\n</figcaption>\n</figure>\n<p>The intersection between a sphere and a plane is a circle. If we look at the sphere from a side view, as shown in Figure 5, we see that the radius can be computed using the Pythagorean theorem (\\(a^2 + b^2 = c^2\\)). The radius of the circle is \\(\\).</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure5.png\" width=\"300\">\n<figcaption>\n<b>Figure 5:</b> A side view of Figure 4. The radius of the circle defined by the intersection can be found using the Pythagorean theorem.\n</figcaption>\n</figure>\n<p>Summing up the area of each cross section from the bottom of the sphere to the top of the sphere gives the volume</p>\n<span class=\"math display\">\\[\\begin{aligned}\nV(r) &amp;= \\int_{-r}^{r} A(\\sqrt{r^2 - h^2})\\; dh\\\\\n     &amp;= \\int_{-r}^{r} \\pi \\sqrt{r^2 - h^2}^2 \\; dh\\\\\n     &amp;= \\frac{4}{3}\\pi r^3.\n\\end{aligned}\\]</span>\n<p>Now that we know the volume of the \\(2\\)-sphere, we can compute the volume of the \\(3\\)-sphere in a similar way. The only difference is where before we used the equation for the area of a circle, we instead use the equation for the volume of the \\(2\\)-sphere. The general formula for the volume of a \\(d\\)-sphere is approximately</p>\n<p><span class=\"math display\">\\[\\begin{equation*}\n\\frac{\\pi^{d/2}}{(d/2+1)!}r^d.\n\\end{equation*}\\]</span></p>\n<p>(Approximately because the denominator should be the <a href=\"https://en.wikipedia.org/wiki/Gamma_function\">Gamma function</a>, but that\u2019s not important for understanding the intuition.)</p>\n<p>Set \\(r = 1\\) and consider the volume of the unit \\(d\\)-sphere as \\(d\\) increases. The plot of the volume is shown in Figure 6.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure6.png\">\n<figcaption>\n<b>Figure 6:</b> The volume of the unit \\(d\\)-sphere goes to 0 as \\(d\\) increases!\n</figcaption>\n</figure>\n<p>The volume of the unit \\(d\\)-sphere goes to 0 as \\(d\\) grows! A high dimensional unit sphere encloses almost no volume! The volume increases from dimensions one to five, but begins decreasing rapidly toward 0 after dimension six.</p>\n<h2 id=\"more-accurate-pictures\">More Accurate Pictures</h2>\n<p>Given the rather unexpected properties of high dimensional cubes and spheres, I hope that you\u2019ll agree that the following are somewhat more accurate pictorial representations.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure7.png\" height=\"200\">\n<figcaption>\n<b>Figure 7:</b> More accurate pictorial representations of high dimensional cubes (left) and spheres (right).\n</figcaption>\n</figure>\n<p>Notice that the corners of the cube are much further away from the center than are the sides. The \\(d\\)-sphere is drawn so that it contains almost no volume but still has radius 1. This image also suggests the next counterintuitive property of high dimensional spheres.</p>\n<h2 id=\"concentration-of-measure\">Concentration of Measure</h2>\n<p>Suppose that you wanted to place a band around the equator of the unit sphere so that, say, 99% of the surface area of the sphere falls within that band. See Figure 8. How large do you think that band would have to be?</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure8.png\" height=\"250\">\n<figcaption>\n<b>Figure 8:</b> In two dimensions a the width of a band around the equator must be very large to contain 99% of the perimeter.\n</figcaption>\n</figure>\n<p>In two dimensions the width of the band needs to be pretty large, indeed nearly 2, to capture 99% of the perimeter of the unit circle. However as the dimension increases the width of the band needed to capture 99% of the surface area gets smaller. In very high dimensional space nearly all of the surface area of the sphere lies a very small distance away from the equator!</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure9.png\">\n<figcaption>\n<b>Figure 9:</b> As the dimension increases the width of the band necessary to capture 99% of the surface area decreases rapidly. Nearly all of the surface area of a high dimensional sphere lies near the equator.\n</figcaption>\n</figure>\n<p>To provide some intuition consider the situation in two dimensions, as shown in Figure 10. For a point on the circle to be close to the equator, its \\(y\\)-coordinate must be small.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure10.png\">\n<figcaption>\n<b>Figure 10:</b> Points near the equator have small y coordinate.\n</figcaption>\n</figure>\n<p>What happens to the values of the coordinates as the dimensions increases? Figure 11 is a plot of 20000 random points sampled uniformly from a \\(d\\)-sphere. As \\(d\\) increases the values become more and more concentrated around 0.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure11.gif\" width=\"750\">\n<figcaption>\n<b>Figure 11:</b> As the dimension increases the coordinates become increasingly concentrated around 0.\n</figcaption>\n</figure>\n<p>Recall that every point on a \\(d\\)-sphere must satisfy the equation \\(x_1^2 + x_2^2 + \\ldots + x_{d+1}^2 = 1\\). Intuitively as \\(d\\) increases the number of terms in the sum increases, and each coordinate gets a smaller share of the single unit, on the average.</p>\n<p>The really weird thing is that any choice of \u201cequator\u201d works! It must, since the sphere is, well, spherically symmetrical. We could have just as easily have chosen any of the options shown in Figure 12.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure12.png\">\n<figcaption>\n<b>Figure 12:</b> Any choice of equator works equally well!\n</figcaption>\n</figure>\n<h2 id=\"kissing-numbers\">Kissing Numbers</h2>\n<p>Consider a unit circle in the plane, shown in Figure 13 in red. The blue circle is said to <em>kiss</em> the red circle if it just barely touches the red circle. (Leave it to mathematicians to think that barely touching is a desirable property of a kiss.) The <em>kissing number</em> is the maximum number of non-overlapping blue circles that can simultaneously kiss the red circle.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure13.png\" width=\"300\">\n<figcaption>\n<b>Figure 13:</b> The kissing number is six in two dimensions.\n</figcaption>\n</figure>\n<p>In two dimensions it\u2019s easy to see that the kissing number is 6. The entire proof is shown in Figure 14. The proof is by contradiction. Assume that more than six non-overlapping blue circles can simultaneously kiss the red circle. We draw the edges from the center of the red circle to the centers of the blue circles, as shown in Figure 14. The angles between these edges must sum to exactly \\(360^{\\circ}\\). Since there are more than six angles, at least one must be less than \\(60^{\\circ}\\). The resulting triangle, shown in Figure 14, is an isosceles triangle. The side opposite the angle that is less than \\(60^{\\circ}\\) must be strictly shorter than the other two sides, which are \\(2r\\) in length. Thus the centers of the two circles must be closer than \\(2r\\) and the circles must overlap, which is a contradiction.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure14.png\">\n<figcaption>\n<b>Figure 14:</b> A proof that the kissing number is six in two dimensions. If more than six blue circles can kiss the red, then one of the angles must be less than 60 degrees. It follows that the two blue circles that form that angle must overlap, which is a contradiction.\n</figcaption>\n</figure>\n<p>It is more difficult to see that in three dimensions the kissing number is 12. Indeed this was famously disputed between Isaac Newton, who correctly thought the kissing number was 12, and David Gregory, who thought it was 13. (Never bet against Newton.) Looking at the optimal configuration, it\u2019s easy to see why Gregory thought it might be possible to fit a 13th sphere in the space between the other 12. As the dimension increases there is suddenly even more space between neighboring spheres and the problem becomes even more difficult.</p>\n<figure align=\"middle\">\n<img src=\"counterintuitive-properties-of-high-dimensional-space/Figure15.png\" width=\"300\">\n<figcaption>\n<b>Figure 15:</b> The kissing number is 12 in three dimensions.\n</figcaption>\n</figure>\n<p>In fact, there are very few dimensions where we know the kissing number exactly. In most dimensions we only have an upper and lower bound on the kissing number, and these bounds can vary by as much as several thousand spheres!</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th>Dimension</th>\n<th style=\"text-align: center;\">Lower Bound</th>\n<th style=\"text-align: right;\">Upper Bound</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>1</strong></td>\n<td style=\"text-align: center;\"><strong>2</strong></td>\n<td style=\"text-align: right;\"><strong>2</strong></td>\n</tr>\n<tr class=\"even\">\n<td><strong>2</strong></td>\n<td style=\"text-align: center;\"><strong>6</strong></td>\n<td style=\"text-align: right;\"><strong>6</strong></td>\n</tr>\n<tr class=\"odd\">\n<td><strong>3</strong></td>\n<td style=\"text-align: center;\"><strong>12</strong></td>\n<td style=\"text-align: right;\"><strong>12</strong></td>\n</tr>\n<tr class=\"even\">\n<td><strong>4</strong></td>\n<td style=\"text-align: center;\"><strong>24</strong></td>\n<td style=\"text-align: right;\"><strong>24</strong></td>\n</tr>\n<tr class=\"odd\">\n<td>5</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: right;\">44</td>\n</tr>\n<tr class=\"even\">\n<td>6</td>\n<td style=\"text-align: center;\">72</td>\n<td style=\"text-align: right;\">78</td>\n</tr>\n<tr class=\"odd\">\n<td>7</td>\n<td style=\"text-align: center;\">126</td>\n<td style=\"text-align: right;\">134</td>\n</tr>\n<tr class=\"even\">\n<td><strong>8</strong></td>\n<td style=\"text-align: center;\"><strong>240</strong></td>\n<td style=\"text-align: right;\"><strong>240</strong></td>\n</tr>\n<tr class=\"odd\">\n<td>9</td>\n<td style=\"text-align: center;\">306</td>\n<td style=\"text-align: right;\">364</td>\n</tr>\n<tr class=\"even\">\n<td>10</td>\n<td style=\"text-align: center;\">500</td>\n<td style=\"text-align: right;\">554</td>\n</tr>\n<tr class=\"odd\">\n<td>11</td>\n<td style=\"text-align: center;\">582</td>\n<td style=\"text-align: right;\">870</td>\n</tr>\n<tr class=\"even\">\n<td>12</td>\n<td style=\"text-align: center;\">840</td>\n<td style=\"text-align: right;\">1357</td>\n</tr>\n<tr class=\"odd\">\n<td>13</td>\n<td style=\"text-align: center;\">1154</td>\n<td style=\"text-align: right;\">2069</td>\n</tr>\n<tr class=\"even\">\n<td>14</td>\n<td style=\"text-align: center;\">1606</td>\n<td style=\"text-align: right;\">3183</td>\n</tr>\n<tr class=\"odd\">\n<td>15</td>\n<td style=\"text-align: center;\">2564</td>\n<td style=\"text-align: right;\">4866</td>\n</tr>\n<tr class=\"even\">\n<td>16</td>\n<td style=\"text-align: center;\">4320</td>\n<td style=\"text-align: right;\">7355</td>\n</tr>\n<tr class=\"odd\">\n<td>17</td>\n<td style=\"text-align: center;\">5346</td>\n<td style=\"text-align: right;\">11072</td>\n</tr>\n<tr class=\"even\">\n<td>18</td>\n<td style=\"text-align: center;\">7398</td>\n<td style=\"text-align: right;\">16572</td>\n</tr>\n<tr class=\"odd\">\n<td>19</td>\n<td style=\"text-align: center;\">10668</td>\n<td style=\"text-align: right;\">24812</td>\n</tr>\n<tr class=\"even\">\n<td>20</td>\n<td style=\"text-align: center;\">17400</td>\n<td style=\"text-align: right;\">36764</td>\n</tr>\n<tr class=\"odd\">\n<td>21</td>\n<td style=\"text-align: center;\">27720</td>\n<td style=\"text-align: right;\">54584</td>\n</tr>\n<tr class=\"even\">\n<td>22</td>\n<td style=\"text-align: center;\">49896</td>\n<td style=\"text-align: right;\">82340</td>\n</tr>\n<tr class=\"odd\">\n<td>23</td>\n<td style=\"text-align: center;\">93150</td>\n<td style=\"text-align: right;\">124416</td>\n</tr>\n<tr class=\"even\">\n<td><strong>24</strong></td>\n<td style=\"text-align: center;\"><strong>196560</strong></td>\n<td style=\"text-align: right;\"><strong>196560</strong></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>As shown in the table, we only know the kissing number exactly in dimensions one through four, eight, and twenty-four. The eight and twenty-four dimensional cases follow from special lattice structures that are known to give optimal packings. In eight dimensions the kissing number is 240, given by the <a href=\"https://en.wikipedia.org/wiki/E8_lattice\">\\(E_{8}\\) lattice</a>. In twenty-four dimensions the kissing number is 196560, given by the <a href=\"https://en.wikipedia.org/wiki/Leech_lattice\">Leech lattice</a>. And not a single sphere more.</p>\n<aside class=\"notice\">\nThis post accompanies a talk given to high school students through Berkeley Splash. Thus intuition is prioritized over mathematical rigor, language is abused, and details are laborious spelled out. If you\u2019re interested in more rigorous treatments of the presented material, please feel free to contact me.\n</aside>\n"}, {"title": "On Computable Functions", "date": "2013-05-17", "description": "What does it mean for a function to be computable? This question captured the imagination of several incredible mathematicians in the early 20th century, and laid the foundation for the field of computer science. In this post I recall a bit of the history surrounding this question.", "slug": "on-computable-functions", "tags": ["computability", "theoretical computer science"], "authors": [{"name": "Marc Khoury"}], "comments": true, "html": "<p>What does it mean to be computable? A function is computable if for a given input its output can be calculated by a finite mechanical procedure. But can we pin this idea down with rigorous mathematics?</p>\n<p>In 1928, David Hilbert <span class=\"citation\" data-cites=\"hilbert1902\">(Hilbert 1902)</span> proposed his famous Entscheidungsproblem, which asks if there is a general procedure for showing that a statement is provable from a given set of axioms. To solve this problem mathematicians first needed to define what it meant to be computable. The first attempt was through primitive recursive functions and was a combined effort by many researchers, including Kurt G\u00f6del, Alonzo Church, Stephen Kleene, Wilhelm Ackermann, John Rosser, and R\u00f3zsa P\u00e9ter.</p>\n<h3 id=\"recursive-functions\">Recursive Functions</h3>\n<p>Primitive recursive functions are defined as a recursive type, starting with a few functions that we assume are computable, called founders, and operators that construct new functions from the founders, called constructors. The founders are the following three functions:</p>\n<ul>\n<li><strong>The constant zero function</strong>: a function that always returns zero</li>\n<li><strong>The successor function</strong>: \\(S(n) = n+1\\)</li>\n<li><strong>The projection function</strong>: \\(\\text{proj}_{n}^m\\) is an \\(m\\)-ary function that returns the \\(n\\)th argument</li>\n</ul>\n<p>Computability theory wasn\u2019t going to get very far if these functions weren\u2019t computable. Next, we have two operations for constructing new functions from old: composition and primitive recursion.</p>\n<ul>\n<li><strong>Composition</strong>: Given a primitive recursive \\(m\\)-ary function \\(h\\) and \\(m\\) \\(n\\)-ary functions \\(g_1,\\ldots, g_m\\), the function \\(f(\\textbf{x}) = h(g_1(\\textbf{x}),\\ldots, g_m(\\textbf{x}))\\) is primitive recursive.</li>\n<li><strong>Primitive Recursion</strong>: Given primitive recursive functions \\(g,h\\) the function\n<span class=\"math display\">\\[\\begin{aligned}\nf(\\textbf{x},0) &amp;= g(\\textbf{x})\\\\\nf(\\textbf{x}, y+1) &amp;= h(\\textbf{x},y,f(\\textbf{x},y))\n\\end{aligned}\\]</span>\nis primitive recursive.</li>\n</ul>\n<p>The set of primitive recursive functions is the set of functions constructed from our three initial functions and closed under composition and primitive recursion. Many familiar functions are primitive recursive: addition, multiplication, exponentiation, primes, max, min, and the logarithm function all fit the bill.</p>\n<p>So are we done? Is every computable function also primitive recursive? Sadly, no: the Ackermann function would be proven in 1928 to be a counterexample.</p>\n<p><span class=\"math display\">\\[\nA(m,n) =\n  \\begin{cases}\n   n+1 &amp; \\text{if } m = 0 \\\\\n   A(m-1,1)  &amp; \\text{if } m &gt; 0 \\text{ and } n = 0\\\\\n   A(m-1,A(m,n-1)) &amp; \\text{if } m &gt; 0 \\text{ and } n &gt; 0\n  \\end{cases}\n\\]</span></p>\n<p>The Ackermann function is a total (defined for all inputs) function that is clearly computable but not primitive recursive. Indeed, in 1928 Ackermann <span class=\"citation\" data-cites=\"ackermann1928\">(Ackermann 1928)</span> showed that his function bounds every primitive recursive function: it grows too fast to be primitive recursive.</p>\n<p>Something was clearly wrong, but early computability theorists didn\u2019t want to abandon primitive recursive functions entirely. What came next was a rather surprising idea at the time: perhaps computable functions need not be total! This was the key that unlocked computability theory: focusing on partial functions, those that may not be defined on all possible inputs.</p>\n<p>The reason for focusing on partial functions is to allow an unbounded search operator. That is, we want to be able to search for the least input value that satisfies a condition and simply be undefined if no such input value exists. This operation is captured by Kleene\u2019s \\(\\mu\\)-operator.</p>\n<ul>\n<li>\\(\\mu\\)-<strong>recursion</strong>: \\(f(x) = (\\mu y)(g(x,y) = 0)\\) returns the least \\(y\\) such that \\(g(x,y) = 0\\) and is undefined if no such \\(y\\) exists. The function \\(g(x,y\u2019)\\) must be defined for all \\(y\u2019 &lt; y\\).</li>\n</ul>\n<p>Taking the closure of the \\(\\mu\\)-operator with all primitive recursive functions gives a class of \\(\\mu\\)-recursive functions. In 1943, Kleene <span class=\"citation\" data-cites=\"kleene1943\">(Kleene 1943)</span> used his \\(\\mu\\)-operator to provide an alternative, but equivalent, definition of general recursive functions. The original definition was given by G\u00f6del in 1934 <span class=\"citation\" data-cites=\"godel1934\">(G\u00f6del 1934)</span>, based on an observation by Jacques Herbrand. It would later be shown that \\(\\mu\\)-recursive functions are the exact same class of functions defined by two competing approaches <span class=\"citation\" data-cites=\"kleene1952\">(Kleene et al. 1952)</span>.</p>\n<h3 id=\"lambda-calculus\">\\(\\lambda\\)-Calculus</h3>\n<p>Simultaneously, from 1931-1934, Church and Kleene were developing \\(\\lambda\\)-calculus as an approach to computable functions. The syntax of \\(\\lambda\\)-calculus defines certain expressions as valid statements, which are called \\(\\lambda\\)-terms. A \\(\\lambda\\)-term is built up from a collection of variables and two operators: abstraction and application.</p>\n<p>Let\u2019s start with a collection of variables \\(x,y,z,\\ldots\\) and suppose \\(M, N\\) are valid \\(\\lambda\\)-terms. The abstraction operator creates the term \\(\\lambda x. M\\), which is a function taking an argument \\(x\\) and returning \\(M\\) with each occurrence of \\(x\\) replaced with the argument. The application operator creates the term \\(M N\\), which represents the application of a function \\(M\\) on input \\(N\\).</p>\n<p>The \\(\\lambda\\)-term \\(\\lambda x.M\\) represents a function \\(f(x) = M\\) and - like recursive functions - many familiar functions are \\(\\lambda\\)-definable. The \\(\\alpha\\)-conversion and \\(\\beta\\)-reduction are classic examples of <em>reductions</em>, which describe how \\(\\lambda\\)-terms are evaluated. An \\(\\alpha\\)-conversion captures the notion that the name of an argument is usually immaterial. For instance \\(\\lambda x.x\\) and \\(\\lambda y.y\\) both represent the identity function and are \\(\\alpha\\)-equivalent. A \\(\\beta\\)-reduction applies a function to its arguments. Take, as an example, the \\(\\lambda\\)-term \\((\\lambda x.x)y\\), which represents the identity function \\((\\lambda x.x)\\) applied to the input \\(y\\). Substituting the argument \\(y\\) for the parameter \\(x\\), the result of the function is \\(y\\). So we say \\((\\lambda x.x)y\\) \\(\\beta\\)-reduces to \\(y\\).</p>\n<p>In 1934 Church proposed that the term \u201ceffectively calculable\u201d be identified with \\(\\lambda\\)-definable. While Church\u2019s formalization of computability would later be shown to be equivalent to Turing\u2019s, G\u00f6del was dissatisfied with Church\u2019s work. In fairness, G\u00f6del also was dissatisfied with his own work! Church would go on to advocate that \u201ceffectively calculable\u201d should be identified with general recursive functions (which G\u00f6del still rejected). In 1936 Church <span class=\"citation\" data-cites=\"church1936\">(Church 1936)</span> published his work proving that that the Entscheidungsproblem was undecidable: there is no general procedure for determining if a statement is provable from a given set of axioms.</p>\n<h3 id=\"turing-machines\">Turing Machines</h3>\n<p>Meanwhile, after hearing about Hilbert\u2019s Entscheidungsproblem, a 22 year old Cambridge student named Alan Turing began working on his own solution to the problem. Turing was unaware of Church\u2019s work at the time, so his approach wasn\u2019t influenced by \\(\\lambda\\)-expressions (this wasn\u2019t the first time Turing failed to perform a literature review). Instead, he envisioned an idealized human agent performing a computation, which he called a \u201ccomputer\u201d. To avoid confusion with the modern definition of computer, we\u2019ll adopt the terminology of Robin Gandy and Wilfried Sieg and use the term \u201ccomputor\u201d to refer to an idealized human agent. The computor had infinite available memory called a tape, essentially an infinite strip of paper, that was divided into squares. The computor could read and write to a square, as well as move from one square to another.</p>\n<p>Turing put several conditions on the computation that the computor could perform. The computor could only have finitely many states (of mind) and the tape could only hold symbols from a finite alphabet. Only a finite number of squares could be observed at a time and the computor could only move to a new square that was at most some finite distance away from an observed square. He also required that any operation must depend only on the current state and the observed symbols, and that there was at most one operation that could be performed per action (his machines were deterministic).</p>\n<p>From this, Turing would go on to define his automatic machines - which would later come to be known as Turing machines - and show the equivalence of the two formalizations. He\u2019d then show that \u201ceffectively calculable\u201d implied computable by his idealized human agent, which in turn implied computable by such a machine. Turing then went on to show that the Entscheidungsproblem was undecidable. Shortly before publishing his work, he learned that Church had already shown that the Entscheidungsproblem was undecidable using \\(\\lambda\\)-calculus. Turing quickly submitted his work in 1936 <span class=\"citation\" data-cites=\"turing1936\">(Turing 1936)</span> - six months after Church - along with a proof demonstrating the equivalence between his machines and \\(\\lambda\\)-calculus.</p>\n<p>After reading Turing\u2019s seminal paper, G\u00f6del was finally convinced that the correct notion of computability had been determined. It would later be shown that all three formalizations - Turing machines, \\(\\mu\\)-recursion, and \\(\\lambda\\)-calculus - actually define the same class of functions. That these three approaches all yielded the same class of functions suggested that mathematicians had captured the correct notion of computation, and supported what would come to be known as the Church-Turing Thesis.</p>\n<p>Three years later, in 1939, Turing completed his Ph.D.\u00a0at Princeton under the supervision of Church. In his thesis he\u2019d state the following <span class=\"citation\" data-cites=\"turing1939\">(Turing 1939)</span>: \u201cWe shall use the expression \u2018computable function\u2019 to mean a function calculable by a machine, and let \u2018effectively calculable\u2019 refer to the intuitive idea without particular identification with any one of these definitions.\u201d</p>\n<blockquote>\n<p><strong>Church-Turing Thesis</strong>: Every effectively calculable function is a computable function.</p>\n</blockquote>\n<p>Church intended for his original thesis to be taken as a definition of what is computable. Likewise, even though he never stated it, Turing had the same intention. In fact, the term \u201cChurch\u2019s Thesis\u201d was coined by Kleene many years after Church had published his work. These days, many people take the Church-Turing Thesis as a definition of what is computable; less formally stating that a function is computable if and only if it can be computed by a Turing machine.</p>\n<p>It\u2019s important to stress that the Church-Turing Thesis is not a definition as many believe. It does not refer to any particular formalization that we\u2019ve discussed and is not a statement that can be formally proven. It is a statement about the nature of computation. Everything that is \u201ceffectively calculable\u201d, in the vague and intuitive sense, is a computable function.</p>\n<div id=\"refs\" class=\"references hanging-indent\" role=\"doc-bibliography\">\n<div id=\"ref-ackermann1928\">\n<p>Ackermann, Wilhelm. 1928. \u201cZum Hilbertschen Aufbau Der Reellen Zahlen.\u201d <em>Mathematische Annalen</em> 99.</p>\n</div>\n<div id=\"ref-church1936\">\n<p>Church, Alonzo. 1936. \u201cAn Unsolvable Problem of Elementary Number Theory.\u201d <em>American Journal of Mathematics</em> 58.</p>\n</div>\n<div id=\"ref-godel1934\">\n<p>G\u00f6del, Kurt. 1934. <em>On Undecidable Propositions of Formal Mathematics Systems</em>. Institute for Advanced Study.</p>\n</div>\n<div id=\"ref-hilbert1902\">\n<p>Hilbert, David. 1902. \u201cMathematical Problems.\u201d <em>Bulletin of the American Mathematical Society</em> 8.</p>\n</div>\n<div id=\"ref-kleene1943\">\n<p>Kleene, Stephen Cole. 1943. \u201cRecursive Predicates and Quantifiers.\u201d <em>Transactions of the American Mathematical Society</em> 53.</p>\n</div>\n<div id=\"ref-kleene1952\">\n<p>Kleene, Stephen Cole, NG De Bruijn, J de Groot, and Adriaan Cornelis Zaanen. 1952. <em>Introduction to Metamathematics</em>. Vol. 483. North-Holland Publishing Company.</p>\n</div>\n<div id=\"ref-turing1936\">\n<p>Turing, Alan. 1936. \u201cOn Computable Numbers, with an Application to the Entscheidungsproblem.\u201d <em>Proceedings of the London Mathematical Society</em>.</p>\n</div>\n<div id=\"ref-turing1939\">\n<p>Turing, Alan Mathison. 1939. \u201cSystems of Logic Based on Ordinals.\u201d <em>Proceedings of the London Mathematical Society</em> 45.</p>\n</div>\n</div>\n"}, {"title": "Numerical Algorithms for Computing Eigenvectors", "date": "2019-02-17", "description": "The eigenvalues and eigenvectors of a matrix are essential accross the sciences. In this post I describe several simple iterative algorithms for computing eigenvector eigenvalue pairs, packed with as much geometric intuition as possible.", "slug": "numerical-algorithms-for-computing-eigenvectors", "tags": ["linear algebra", "numerical algorithms", "rayleigh quotient iteration"], "authors": [{"name": "Marc Khoury"}], "comments": true, "html": "<p>The eigenvalues and eigenvectors of a matrix are essential in many applications across the sciences. Despite their utility, students often leave their linear algebra courses with very little intuition for eigenvectors. In this post we describe several surprisingly simple algorithms for computing the eigenvalues and eigenvectors of a matrix, while attempting to convey as much geometric intuition as possible.</p>\n<p>Let \\(A\\) be a symmetric positive definite matrix. Since \\(A\\) is symmetric all of the eigenvalues of \\(A\\) are real and \\(A\\) has a full set of orthogonal eigenvectors. Let \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n &gt; 0\\) denote the eigenvalues of \\(A\\) and let \\(u_{1}, \\ldots, u_n\\) denote their corresponding eigenvectors. The fact that \\(A\\) is positive definite means that \\(\\lambda_i &gt; 0\\) for all \\(i\\). This condition isn\u2019t strictly necessary for the algorithms described below; I\u2019m assuming it so that I can refer to the largest eigenvalue as opposed to the largest in magnitude eigenvalue.</p>\n<p>All of my intuition for positive definite matrices comes from the geometry of the quadratic form \\(x^{\\top}Ax\\). Figure 1 plots \\(x^{\\top}Ax\\) in \\(\\mathbb{R}^3\\) for several \\(2 \\) matrices. When \\(A\\) is positive definite, the quadratic form \\(x^{\\top}Ax\\) is shaped like a bowl. More rigorously it has positive curvature in every direction and the curvature at the origin in the direction of each eigenvector is proportional to the eigenvalue of that eigenvector. In \\(\\mathbb{R}^3\\), the two eigenvectors give the directions of the maximum and minimum curvature at the origin. These are also known as principal directions in differential geometry, and the curvatures in these directions are known as principal curvatures. I often shorten this intuition by simply stating that positive definite matrices <em>are</em> bowls, because this is always the picture I have in my head when discussing them.</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure1v2.png\" width=\"750\">\n<figcaption>\n<b>Figure 1:</b> The geometry of the quadratic form \\(x^{\\top}Ax\\) for, from left to right, a positive definite matrix, a positive semi-definite matrix, an indefinite matrix, and a negative definite matrix. When \\(A\\) is positive definite it has positive curvature in every direction and is shaped like a bowl. The curvature at the origin in the direction of an eigenvector is proportional to the eigenvalue. A positive semi-definite matrix may have one or more eigenvalues equal to 0. This creates a flat (zero curvature) subspace of dimension equal to the number of eigenvalues with value equal to 0. An indefinite matrix has both positive and negative eigenvalues, and so has some directions with positive curvature and some with negative curvature, creating a saddle. A negative definite matrix has all negative eigenvalues and so the curvature in every direction is negative at every point.\n</figcaption>\n</figure>\n<p>Now suppose we wanted to compute a single eigenvector of \\(A\\). This problem comes up more often than you\u2019d think and it\u2019s a crime that undergraduate linear algebra courses don\u2019t often make this clear. The first algorithm that one generally learns, and the only algorithm in this post that I knew as an undergraduate, is an incredibly simple algorithm called Power Iteration. Starting from a random unit vector \\(v\\) we simply compute \\(A^{t}v\\) iteratively. For sufficiently large \\(t\\), \\(A^{t}v\\) converges to the eigenvector corresponding to the largest eigenvalue of \\(A\\), hereafter referred to as the \u201ctop eigenvector\u201d.</p>\n<pre><code class=\"language-python\">\ndef power_iteration(A, max_iter):\n  v = np.random.randn(A.shape[0])\n  v /= np.linalg.norm(v) #generate a uniformly random unit vector\n  for t in range(max_iter):\n    v = np.dot(A, v) #compute Av\n    v /= np.linalg.norm(v)\n  return v\n</code></pre>\n<p>To see why Power Iteration converges to the top eigenvector of \\(A\\) it helps to write \\(v\\) in the eigenbasis of \\(A\\) as \\(v = \\sum_{i=1}^n\\beta_{i}u_{i}\\) for some coefficients \\(\\beta_i\\). Then we have that</p>\n<span class=\"math display\">\\[\\begin{aligned}\nA^{t}v &amp;= A^{t}(\\sum_{i= 1}^{n}\\beta_{i}u_{i})\\\\\n       &amp;= \\sum_{i=1}^{n}\\beta_{i}A^{t}u_{i}\\\\\n       &amp;= \\sum_{i=1}^{n}\\beta_{i}\\lambda_{i}^{t}u_{i}\\\\\n       &amp;= \\lambda_{1}^t \\sum_{i=1}^{n}\\beta_{i}\\left(\\frac{\\lambda_{i}}{\\lambda_{1}}\\right)^t u_{i}\\\\\n       &amp;= \\lambda_{1}^{t} \\left( \\beta_1 u_1 + \\sum_{i=2}^{n}\\beta_{i}\\left(\\frac{\\lambda_{i}}{\\lambda_{1}}\\right)^t u_{i} \\right).\n\\end{aligned}\\]</span>\n<p>Since \\(\\lambda_1\\) is the largest eigenvalue, the fractions \\(\\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^t\\) go to 0 as \\(t \\rightarrow \\infty\\), for all \\( i \\neq 1\\). Thus the only component of \\(A^{t}v\\) that has any weight is that of \\(u_1\\). How quickly each of those terms goes to 0 depends on the ratio \\(\\frac{\\lambda_{2}}{\\lambda_{1}}\\). If this term is close to 1 then it may take many iterations to disambiguate between the top two (or more) eigenvectors. We say that the Power Iteration algorithm converges at a rate of \\(O\\left(\\left(\\frac{\\lambda_{2}}{\\lambda_{1}}\\right)^t\\right)\\), which for some unfortunate historical reason is referred to as \u201clinear convergence\u201d.</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure2.gif\" width=\"550\">\n<figcaption>\n<b>Figure 2:</b> An illustration of the Power Iteration algorithm. The \\(i\\)th bar represents the component of the current iterate on the \\(i\\)th eigenvector, in order of decreasing eigenvalue. Notice that the components corresponding to the smallest eigenvalues decrease most rapidly, whereas the components on the largest eigenvalues take longer to converge. This animation represents 50 iterations of Power Iteration.\n</figcaption>\n</figure>\n<p>Power Iteration will give us an estimate of the top eigenvector \\(u_1\\), but what about the other extreme? What if instead we wanted to compute \\(u_n\\), the eigenvector corresponding to the smallest eigenvalue? It turns out there is a simple modification to the standard Power Iteration algorithm that computes \\(u_n\\). Instead of multiplying by \\(A\\) at each iteration, multiply by \\(A^{-1}\\). This works because the eigenvalues of \\(A^{-1}\\) are \\(\\frac{1}{\\lambda_i}\\), and thus the smallest eigenvalue of \\(A\\), \\(\\lambda_n\\), corresponds to the largest eigenvalue of \\(A^{-1}\\), \\(\\frac{1}{\\lambda_{n}}\\). Furthermore the eigenvectors of \\(A^{-1}\\) are unchanged. This slight modification is called Inverse Iteration, and it exhibits the same convergence as Power Iteration, by the same analysis.</p>\n<pre><code class=\"language-python\">\ndef inverse_iteration(A, max_iter):\n  v = np.random.randn(A.shape[0])\n  v /= np.linalg.norm(v) #generate a uniformly random unit vector\n  lu, piv = scipy.linalg.lu_factor(A) # compute LU factorization of A\n  for t in range(max_iter):\n    v = scipy.linalg.lu_solve((lu, piv), v) #compute A^(-1)v\n    v /= np.linalg.norm(v)\n  return v\n</code></pre>\n<p>Note that we don\u2019t actually compute \\(A^{-1}\\) explicitly. Instead we compute an LU factorization of \\(A\\) and solve the system \\(LUv_{t+1} = v_{t}\\). The matrix that we\u2019re multiplying by does not change at each iteration, so we can compute the LU factorization once and quickly solve a linear system to compute \\(A^{-1}v\\) at each iteration.</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure3.gif\" width=\"550\">\n<figcaption>\n<b>Figure 3:</b> The Inverse Iteration algorithm. Notice that in this case the algorithm converges to the eigenvector corresponding to the smallest eigenvalue.\n</figcaption>\n</figure>\n<p>Power Iteration and Inverse Iteration find the eigenvectors at the extremes of the spectrum of \\(A\\), but sometimes we may want to compute a specific eigenvector corresponding to a specific eigenvalue. Suppose that we have an estimate \\(\\mu\\) of an eigenvalue. We can find the eigenvector corresponding to the eigenvalue of \\(A\\) closest to \\(\\mu\\) by a simple modification to Inverse Iteration. Instead of multiplying by \\(A^{-1}\\) at each iteration, multiply by \\((\\mu I_{n} - A)^{-1}\\) where \\(I_{n}\\) is the identity matrix. The eigenvalues of \\((\\mu I_{n} - A)^{-1}\\) are \\(\\frac{1}{\\mu - \\lambda_{i}}\\). Thus the largest eigenvalue of \\((\\mu I_{n} - A)^{-1}\\) corresponds to the eigenvalue of \\(A\\) whose value is closest to \\(\\mu\\). By the same analysis as Power Iteration, Shifted Inverse Iteration also exhibits linear convergence. However the better the estimate \\(\\mu\\) the larger \\(\\frac{1}{\\mu - \\lambda_{i}}\\) and, consequently, the faster the convergence.</p>\n<pre><code class=\"language-python\">\ndef shifted_inverse_iteration(A, mu, max_iter):\n  I = np.identity(A.shape[0])\n  v = np.random.randn(A.shape[0])\n  v /= np.linalg.norm(v) #generate a uniformly random unit vector\n  lu, piv = scipy.linalg.lu_factor(mu*I - A) # compute LU factorization of (mu*I - A)\n  for t in range(max_iter):\n    v = scipy.linalg.lu_solve((lu, piv), v) #compute (mu*I - A)^(-1)v\n    v /= np.linalg.norm(v)\n  return v\n</code></pre>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure4.gif\" width=\"550\">\n<figcaption>\n<b>Figure 4:</b> The Shifted Inverse Iteration algorithm. In this case we converge to the eigenvector corresponding to the eigenvalue nearest \\(\\mu\\).\n</figcaption>\n</figure>\n<p>Shifted Inverse Iteration converges quickly if a good estimate of the target eigenvalue is available. However if \\(\\mu\\) is a poor approximation of the desired eigenvalue, Shifted Inverse Iteration may take a long time to converge. In fact all of the algorithms we\u2019ve presented so far have exactly the same convergence rate; they all converge linearly. If instead we could improve on the eigenvalue estimate at each iteration we could potentially develop an algorithm with a faster convergence rate. This is the main idea behind Rayleigh Quotient Iteration.</p>\n<p>The Rayleigh quotient is defined as \\[ \\lambda_{R}(v) = \\frac{v^{\\top} Av }{ v^{\\top}v} \\] for any vector \\(v\\). There are many different ways in which we can understand the Rayleigh quotient. Some intuition that is often given is that the Rayleigh quotient is the scalar value that behaves most like an \u201ceigenvalue\u201d for \\(v\\), even though \\(v\\) may not be an eigenvector. What is meant is that the Rayleigh quotient is the minimum to the optimization problem \\[ \\min_{\\lambda \\in \\mathbb{R}} \\|Av - \\lambda v\\|^2. \\] This intuition is hardly satisfying.</p>\n<p>Let\u2019s return to the geometry of the quadratic forms \\(x^{\\top}Ax\\) and \\(x^{\\top}x\\) which comprise the Rayleigh quotient, drawn in orange and blue respectively in Figure 5. Without loss of generality we can assume that \\(A\\) is a diagonal matrix. (This is without loss of generality because we\u2019re merely rotating the surface so that the eigenvectors align with the \\(x\\) and \\(y\\) axes, which does not affect the geometry of the surface. This is a common trick in the numerical algorithms literature.) In this coordinate system, the quadratic form \\(x^{\\top}Ax = \\lambda_1x_1^2 + \\lambda_2 x_2^2\\), where \\(\\lambda_1\\) and \\(\\lambda_2\\) are the diagonal entries, and thus the eigenvalues, of \\(A\\).</p>\n<p>Consider any vector \\(v\\) and let \\(h = \\operatorname{span}\\{v, (0,0,1)\\}\\) be the plane spanned by \\(v\\) and the vector \\((0,0,1)\\). The intersection of \\(h\\) with the quadratic forms \\(x^{\\top}Ax\\) and \\(x^{\\top}x\\) is comprised of two parabolas, also shown in Figure 5. (This is a common trick in the geometric algorithms literature.) If \\(v\\) is aligned with the \\(x\\)-axis, then, within the coordinate system defined by \\(h\\), \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\lambda_1 x^2\\) and \\(x^{\\top}x\\) can be parameterized by \\(y = x^2\\). (Note that here \\(y\\) and \\(x\\) refer to local coordinates within \\(h\\) and are distinct from the vector \\(x\\) used in \\(x^{\\top}Ax\\).) Similarly if \\(v\\) is aligned with the \\(y\\)-axis, then \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\lambda_2 x^2\\). (If \\(v\\) is any other vector then \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\kappa x^2\\) for some \\(\\kappa\\) dependent upon \\(v\\).) The Rayleigh quotient at \\(v\\) is \\(\\lambda_{R}(v) = \\frac{\\lambda_1 x^2 }{ x^2} = \\lambda_1\\). The curvature of the parabola \\(y = \\lambda_1 x^2\\) at the origin is \\(2\\lambda_1\\). Thus the Rayleigh quotient is proportional to the the curvature of \\(x^{\\top}Ax\\) in the direction \\(v\\)!</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure5.png\" width=\"750\">\n<figcaption>\n<b>Figure 5:</b> The quadratic form \\(x^{\\top}Ax\\) is shown in orange and \\(x^{\\top} x\\) is shown in blue. Intersecting both surfaces with a plane \\(h\\) gives two parabola. Within the plane \\(h\\) we can define a local coordinate system and parameterize both parabola as \\(\\kappa x^2\\) and \\(x^2\\). The Rayleigh quotient is equal to the ratio of the heights of the parabolas at any point, which is always equal to \\(\\kappa\\).\n</figcaption>\n</figure>\n<p>From this intuition it is clear that the value of the Rayleigh quotient is identical along any ray starting at, but not including, the origin. The length of \\(v\\) corresponds to the value of \\(x\\) in the coordinate system defined by \\(h\\), which does not affect the Rayleigh quotient. We can also see this algebraically, by choosing a unit vector \\(v\\) and parameterizing a ray in the direction \\(v\\) as \\(\\alpha v\\) for \\(\\alpha \\in \\mathbb{R}\\) and \\(\\alpha &gt; 0\\). Then we have that</p>\n<span class=\"math display\">\\[\\begin{aligned}\n\\lambda_{R}(\\alpha v) &amp;= \\frac{(\\alpha v^{\\top})A(\\alpha v)} {\\alpha^2 v^{\\top}v}\\\\\n                      &amp;= \\frac{v^{\\top}Av} {v^{\\top}v}\\\\\n                      &amp;= v^{\\top}Av.\n\\end{aligned}\\]</span>\n<p>Thus it is sufficient to consider the values of the Rayleigh quotient on the unit sphere.</p>\nFor a unit vector \\(v\\) the value of the Rayleigh quotient can be written in the eigenbasis as\n<span class=\"math display\">\\[\\begin{aligned}\nv^{\\top}Av = \\sum_{i=1}^{n} \\lambda_{i}\\langle v, u_i\\rangle^2\n\\end{aligned}\\]</span>\n<p>where \\(\\sum_{i=1}^{n} \\langle v, u_i\\rangle^2 = 1\\). Thus the Rayleigh quotient is a convex combination of the eigenvalues of \\(A\\) and so its value is bounded by the minimum and maximum eigenvalues \\( \\lambda_{n} \\leq \\lambda_{R}(v) \\leq \\lambda_{1}\\) for all \\(v\\). This fact is also easily seen from the geometric picture above, as the curvature at the origin is bounded by twice the minimum and maximum eigenvalues. It can be readily seen by either direct calculation or by the coefficients of the convex combination, that if \\(v\\) is an eigenvector, then \\(\\lambda_{R}(v)\\) is the corresponding eigenvalue of \\(v\\).</p>\n<p>Recall that a critical point of a function is a point where the derivative is equal to 0. It should come as no surprise that the eigenvalues are the critical values of the Rayleigh quotient and the eigenvectors are the critical points. What is less obvious is the special geometric structure of the critical points.</p>\n<p>The gradient of the Rayleigh quotient is \\(\\frac{2}{v^{\\top}v}(Av - \\lambda_{R}(v)v)\\), from which it is easy to see that every eigenvector is a critical point of \\(\\lambda_{R}\\). The type of critical point is determined by the Hessian matrix, which at the critical point \\(u_i\\) is \\(2(A - \\lambda_{i}I)\\). The eigenvalues of the Hessian are \\(\\lambda_j - \\lambda_i\\) for \\(j \\in [1,n]\\). Assuming for a moment that the eigenvalues are all distinct, the matrix \\(2(A - \\lambda_{i}I)\\) has \\(i-1\\) eigenvectors that are positive, one eigenvalue that is 0, and \\(n - i\\) eigenvalues that are negative. The 0 eigenvalue represents the fact that the value of the Rayleigh quotient is unchanged along the ray \\(\\alpha u_i\\). The other eigenvalues represent the fact that at \\(u_i\\), along the unit sphere, there are \\(i-1\\) directions in which we can walk to increase the value of the Rayleigh quotient, and \\(n-i\\) directions that decrease the Rayleigh quotient. Thus each eigenvector gives rise to a different type of saddle, and there are exactly two critical points of each type on the unit sphere.</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure6.png\" style=\"width: 400px; margin:auto;\">\n<figcaption>\n<b>Figure 6:</b> Contours of the Rayleigh quotient on the unit sphere and the gradient of the Rayleigh quotient at each point. We clearly see one minimum in blue corresponding to the minimum eigenvalue, one saddle point, and one maximum in bright yellow corresponding to the maximum eigenvalue.\n</figcaption>\n</figure>\n<p>Finally we come to the crown jewel of the algorithms in this post. The Rayleigh Quotient Iteration algorithm simply updates the estimate \\(\\mu\\) at each iteration with the Rayleigh quotient. Other than this slight modification, the algorithm is exactly like Shifted Inverse iteration.</p>\n<pre><code class=\"language-python\">\ndef rayleigh_quotient_iteration(A, max_iter):\n  I = np.identity(A.shape[0])\n  v = np.random.randn(A.shape[0])\n  v /= np.linalg.norm(v) #generate a uniformly random unit vector\n  mu = np.dot(v, np.dot(A, v))\n  for t in range(max_iter):\n    v = np.linalg.solve(mu * I - A, v) #compute (mu*I - A)^(-1)v\n    v /= np.linalg.norm(v)\n    mu = np.dot(v, np.dot(A, v)) #compute Rayleigh quotient\n  return (v, mu)\n</code></pre>\n<p>This slight modification drastically improves the convergence rate. Unlike the other algorithms in this post which converge linearly, Rayleigh quotient iteration exhibits local cubic convergence! This means that, assuming \\(\\| v_{t} - u_i\\| \\leq \\epsilon\\) for some \\(u_i\\), on the next iteration we will have that \\(\\| v_{t+1} - u_{i} \\| \\leq \\epsilon^3\\). In practice this means that you should expect triple the number of correct digits at each iteration. It\u2019s hard to understate how crazy fast cubic convergence is, and, to the best of the author\u2019s knowledge, algorithms that exhibit cubic convergence are rare in the numerical algorithms literature.</p>\n<figure align=\"middle\">\n<img src=\"numerical-algorithms-for-computing-eigenvectors/Figure7v2.gif\" width=\"550\">\n<figcaption>\n<b>Figure 7:</b> The Rayleigh Quotient Iteration algorithm. After only 6 iterations the eigenvalue estimate \\(\\mu_t\\) is so accurate that the resulting matrix \\((\\mu_t I_{n} - A)\\) is singular up-to machine precision and we can no longer solve the system for an inverse. Note that every other figure in this post shows 50 iterations.\n</figcaption>\n</figure>\n<p>Intuitively, the reason that Rayleigh Quotient Iteration exhibits cubic convergence is because, while the Shifted Inverse Iteration step converges linearly, the Rayleigh quotient is a quadratically good estimate of an eigenvalue near an eigenvector. To see this consider the Taylor series expansion of \\(\\lambda_{R}\\) near an eigenvector \\(u_i\\).</p>\n<span class=\"math display\">\\[\\begin{aligned}\n\\lambda_{R}(v) &amp;= \\lambda_{R}(u_i) + (v - u_{i})^{\\top} \\nabla \\lambda_{R}(u_i) + O(||v - u_i||^2)\\\\\n               &amp;= \\lambda_{R}(u_i) + O(||v - u_i||^2)\\\\\n\\lambda_{R}(v) - \\lambda_{R}(u_i) &amp;= O(||v - u_i||^2)                \n\\end{aligned}\\]</span>\n<p>The second step follows from the fact that \\(u_i\\) is a critical point of \\(\\lambda_{R}\\) and so \\(\\nabla \\lambda_{R}(u_i) = 0\\).</p>\n<p>While Rayleigh Quotient Iteration exhibits very fast convergence, it\u2019s not without its drawbacks. First, notice that the system \\((\\mu_{t}I - A)^{-1}\\) changes at each iteration. Thus we cannot precompute a factorization of this matrix and quickly solve the system using forward and backward substitution at each iteration, like we did in the Shifted Inverse Iteration algorithm. We need to solve a different linear system at each iteration, which is much more expensive. Second, Rayleigh Quotient Iteration gives no control over to which eigenvector it converges. The eigenvector it converges to depends on which basin of attraction the initial random vector \\(v_{0}\\) falls into. Thus cubic convergence comes at a steep cost. This balance between an improved convergence rate and solving a different linear system at each iteration feels like mathematical poetic justice. The price to pay for cubic convergence is steep.</p>\n"}]