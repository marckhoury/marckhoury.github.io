<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Numerical Algorithms for Computing Eigenvectors",date:"2019-02-17T00:00:00.000Z",description:"The eigenvalues and eigenvectors of a matrix are essential accross the sciences. In this post I describe several simple iterative algorithms for computing eigenvector eigenvalue pairs, packed with as much geometric intuition as possible.",slug:"numerical-algorithms-for-computing-eigenvectors",tags:["linear algebra","numerical algorithms","rayleigh quotient iteration"],authors:[{name:"Marc Khoury"}],comments:true,html:"\u003Cp\u003EThe eigenvalues and eigenvectors of a matrix are essential in many applications across the sciences. Despite their utility, students often leave their linear algebra courses with very little intuition for eigenvectors. In this post we describe several surprisingly simple algorithms for computing the eigenvalues and eigenvectors of a matrix, while attempting to convey as much geometric intuition as possible.\u003C\u002Fp\u003E\n\u003Cp\u003ELet \\(A\\) be a symmetric positive definite matrix. Since \\(A\\) is symmetric all of the eigenvalues of \\(A\\) are real and \\(A\\) has a full set of orthogonal eigenvectors. Let \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n &gt; 0\\) denote the eigenvalues of \\(A\\) and let \\(u_{1}, \\ldots, u_n\\) denote their corresponding eigenvectors. The fact that \\(A\\) is positive definite means that \\(\\lambda_i &gt; 0\\) for all \\(i\\). This condition isn’t strictly necessary for the algorithms described below; I’m assuming it so that I can refer to the largest eigenvalue as opposed to the largest in magnitude eigenvalue.\u003C\u002Fp\u003E\n\u003Cp\u003EAll of my intuition for positive definite matrices comes from the geometry of the quadratic form \\(x^{\\top}Ax\\). Figure 1 plots \\(x^{\\top}Ax\\) in \\(\\mathbb{R}^3\\) for several \\(2 \\) matrices. When \\(A\\) is positive definite, the quadratic form \\(x^{\\top}Ax\\) is shaped like a bowl. More rigorously it has positive curvature in every direction and the curvature at the origin in the direction of each eigenvector is proportional to the eigenvalue of that eigenvector. In \\(\\mathbb{R}^3\\), the two eigenvectors give the directions of the maximum and minimum curvature at the origin. These are also known as principal directions in differential geometry, and the curvatures in these directions are known as principal curvatures. I often shorten this intuition by simply stating that positive definite matrices \u003Cem\u003Eare\u003C\u002Fem\u003E bowls, because this is always the picture I have in my head when discussing them.\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure1v2.png\" width=\"750\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 1:\u003C\u002Fb\u003E The geometry of the quadratic form \\(x^{\\top}Ax\\) for, from left to right, a positive definite matrix, a positive semi-definite matrix, an indefinite matrix, and a negative definite matrix. When \\(A\\) is positive definite it has positive curvature in every direction and is shaped like a bowl. The curvature at the origin in the direction of an eigenvector is proportional to the eigenvalue. A positive semi-definite matrix may have one or more eigenvalues equal to 0. This creates a flat (zero curvature) subspace of dimension equal to the number of eigenvalues with value equal to 0. An indefinite matrix has both positive and negative eigenvalues, and so has some directions with positive curvature and some with negative curvature, creating a saddle. A negative definite matrix has all negative eigenvalues and so the curvature in every direction is negative at every point.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003ENow suppose we wanted to compute a single eigenvector of \\(A\\). This problem comes up more often than you’d think and it’s a crime that undergraduate linear algebra courses don’t often make this clear. The first algorithm that one generally learns, and the only algorithm in this post that I knew as an undergraduate, is an incredibly simple algorithm called Power Iteration. Starting from a random unit vector \\(v\\) we simply compute \\(A^{t}v\\) iteratively. For sufficiently large \\(t\\), \\(A^{t}v\\) converges to the eigenvector corresponding to the largest eigenvalue of \\(A\\), hereafter referred to as the “top eigenvector”.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\ndef power_iteration(A, max_iter):\n  v = np.random.randn(A.shape[0])\n  v \u002F= np.linalg.norm(v) #generate a uniformly random unit vector\n  for t in range(max_iter):\n    v = np.dot(A, v) #compute Av\n    v \u002F= np.linalg.norm(v)\n  return v\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ETo see why Power Iteration converges to the top eigenvector of \\(A\\) it helps to write \\(v\\) in the eigenbasis of \\(A\\) as \\(v = \\sum_{i=1}^n\\beta_{i}u_{i}\\) for some coefficients \\(\\beta_i\\). Then we have that\u003C\u002Fp\u003E\n\u003Cspan class=\"math display\"\u003E\\[\\begin{aligned}\nA^{t}v &amp;= A^{t}(\\sum_{i= 1}^{n}\\beta_{i}u_{i})\\\\\n       &amp;= \\sum_{i=1}^{n}\\beta_{i}A^{t}u_{i}\\\\\n       &amp;= \\sum_{i=1}^{n}\\beta_{i}\\lambda_{i}^{t}u_{i}\\\\\n       &amp;= \\lambda_{1}^t \\sum_{i=1}^{n}\\beta_{i}\\left(\\frac{\\lambda_{i}}{\\lambda_{1}}\\right)^t u_{i}\\\\\n       &amp;= \\lambda_{1}^{t} \\left( \\beta_1 u_1 + \\sum_{i=2}^{n}\\beta_{i}\\left(\\frac{\\lambda_{i}}{\\lambda_{1}}\\right)^t u_{i} \\right).\n\\end{aligned}\\]\u003C\u002Fspan\u003E\n\u003Cp\u003ESince \\(\\lambda_1\\) is the largest eigenvalue, the fractions \\(\\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^t\\) go to 0 as \\(t \\rightarrow \\infty\\), for all \\( i \\neq 1\\). Thus the only component of \\(A^{t}v\\) that has any weight is that of \\(u_1\\). How quickly each of those terms goes to 0 depends on the ratio \\(\\frac{\\lambda_{2}}{\\lambda_{1}}\\). If this term is close to 1 then it may take many iterations to disambiguate between the top two (or more) eigenvectors. We say that the Power Iteration algorithm converges at a rate of \\(O\\left(\\left(\\frac{\\lambda_{2}}{\\lambda_{1}}\\right)^t\\right)\\), which for some unfortunate historical reason is referred to as “linear convergence”.\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure2.gif\" width=\"550\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 2:\u003C\u002Fb\u003E An illustration of the Power Iteration algorithm. The \\(i\\)th bar represents the component of the current iterate on the \\(i\\)th eigenvector, in order of decreasing eigenvalue. Notice that the components corresponding to the smallest eigenvalues decrease most rapidly, whereas the components on the largest eigenvalues take longer to converge. This animation represents 50 iterations of Power Iteration.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EPower Iteration will give us an estimate of the top eigenvector \\(u_1\\), but what about the other extreme? What if instead we wanted to compute \\(u_n\\), the eigenvector corresponding to the smallest eigenvalue? It turns out there is a simple modification to the standard Power Iteration algorithm that computes \\(u_n\\). Instead of multiplying by \\(A\\) at each iteration, multiply by \\(A^{-1}\\). This works because the eigenvalues of \\(A^{-1}\\) are \\(\\frac{1}{\\lambda_i}\\), and thus the smallest eigenvalue of \\(A\\), \\(\\lambda_n\\), corresponds to the largest eigenvalue of \\(A^{-1}\\), \\(\\frac{1}{\\lambda_{n}}\\). Furthermore the eigenvectors of \\(A^{-1}\\) are unchanged. This slight modification is called Inverse Iteration, and it exhibits the same convergence as Power Iteration, by the same analysis.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\ndef inverse_iteration(A, max_iter):\n  v = np.random.randn(A.shape[0])\n  v \u002F= np.linalg.norm(v) #generate a uniformly random unit vector\n  lu, piv = scipy.linalg.lu_factor(A) # compute LU factorization of A\n  for t in range(max_iter):\n    v = scipy.linalg.lu_solve((lu, piv), v) #compute A^(-1)v\n    v \u002F= np.linalg.norm(v)\n  return v\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ENote that we don’t actually compute \\(A^{-1}\\) explicitly. Instead we compute an LU factorization of \\(A\\) and solve the system \\(LUv_{t+1} = v_{t}\\). The matrix that we’re multiplying by does not change at each iteration, so we can compute the LU factorization once and quickly solve a linear system to compute \\(A^{-1}v\\) at each iteration.\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure3.gif\" width=\"550\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 3:\u003C\u002Fb\u003E The Inverse Iteration algorithm. Notice that in this case the algorithm converges to the eigenvector corresponding to the smallest eigenvalue.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EPower Iteration and Inverse Iteration find the eigenvectors at the extremes of the spectrum of \\(A\\), but sometimes we may want to compute a specific eigenvector corresponding to a specific eigenvalue. Suppose that we have an estimate \\(\\mu\\) of an eigenvalue. We can find the eigenvector corresponding to the eigenvalue of \\(A\\) closest to \\(\\mu\\) by a simple modification to Inverse Iteration. Instead of multiplying by \\(A^{-1}\\) at each iteration, multiply by \\((\\mu I_{n} - A)^{-1}\\) where \\(I_{n}\\) is the identity matrix. The eigenvalues of \\((\\mu I_{n} - A)^{-1}\\) are \\(\\frac{1}{\\mu - \\lambda_{i}}\\). Thus the largest eigenvalue of \\((\\mu I_{n} - A)^{-1}\\) corresponds to the eigenvalue of \\(A\\) whose value is closest to \\(\\mu\\). By the same analysis as Power Iteration, Shifted Inverse Iteration also exhibits linear convergence. However the better the estimate \\(\\mu\\) the larger \\(\\frac{1}{\\mu - \\lambda_{i}}\\) and, consequently, the faster the convergence.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\ndef shifted_inverse_iteration(A, mu, max_iter):\n  I = np.identity(A.shape[0])\n  v = np.random.randn(A.shape[0])\n  v \u002F= np.linalg.norm(v) #generate a uniformly random unit vector\n  lu, piv = scipy.linalg.lu_factor(mu*I - A) # compute LU factorization of (mu*I - A)\n  for t in range(max_iter):\n    v = scipy.linalg.lu_solve((lu, piv), v) #compute (mu*I - A)^(-1)v\n    v \u002F= np.linalg.norm(v)\n  return v\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure4.gif\" width=\"550\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 4:\u003C\u002Fb\u003E The Shifted Inverse Iteration algorithm. In this case we converge to the eigenvector corresponding to the eigenvalue nearest \\(\\mu\\).\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EShifted Inverse Iteration converges quickly if a good estimate of the target eigenvalue is available. However if \\(\\mu\\) is a poor approximation of the desired eigenvalue, Shifted Inverse Iteration may take a long time to converge. In fact all of the algorithms we’ve presented so far have exactly the same convergence rate; they all converge linearly. If instead we could improve on the eigenvalue estimate at each iteration we could potentially develop an algorithm with a faster convergence rate. This is the main idea behind Rayleigh Quotient Iteration.\u003C\u002Fp\u003E\n\u003Cp\u003EThe Rayleigh quotient is defined as \\[ \\lambda_{R}(v) = \\frac{v^{\\top} Av }{ v^{\\top}v} \\] for any vector \\(v\\). There are many different ways in which we can understand the Rayleigh quotient. Some intuition that is often given is that the Rayleigh quotient is the scalar value that behaves most like an “eigenvalue” for \\(v\\), even though \\(v\\) may not be an eigenvector. What is meant is that the Rayleigh quotient is the minimum to the optimization problem \\[ \\min_{\\lambda \\in \\mathbb{R}} \\|Av - \\lambda v\\|^2. \\] This intuition is hardly satisfying.\u003C\u002Fp\u003E\n\u003Cp\u003ELet’s return to the geometry of the quadratic forms \\(x^{\\top}Ax\\) and \\(x^{\\top}x\\) which comprise the Rayleigh quotient, drawn in orange and blue respectively in Figure 5. Without loss of generality we can assume that \\(A\\) is a diagonal matrix. (This is without loss of generality because we’re merely rotating the surface so that the eigenvectors align with the \\(x\\) and \\(y\\) axes, which does not affect the geometry of the surface. This is a common trick in the numerical algorithms literature.) In this coordinate system, the quadratic form \\(x^{\\top}Ax = \\lambda_1x_1^2 + \\lambda_2 x_2^2\\), where \\(\\lambda_1\\) and \\(\\lambda_2\\) are the diagonal entries, and thus the eigenvalues, of \\(A\\).\u003C\u002Fp\u003E\n\u003Cp\u003EConsider any vector \\(v\\) and let \\(h = \\operatorname{span}\\{v, (0,0,1)\\}\\) be the plane spanned by \\(v\\) and the vector \\((0,0,1)\\). The intersection of \\(h\\) with the quadratic forms \\(x^{\\top}Ax\\) and \\(x^{\\top}x\\) is comprised of two parabolas, also shown in Figure 5. (This is a common trick in the geometric algorithms literature.) If \\(v\\) is aligned with the \\(x\\)-axis, then, within the coordinate system defined by \\(h\\), \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\lambda_1 x^2\\) and \\(x^{\\top}x\\) can be parameterized by \\(y = x^2\\). (Note that here \\(y\\) and \\(x\\) refer to local coordinates within \\(h\\) and are distinct from the vector \\(x\\) used in \\(x^{\\top}Ax\\).) Similarly if \\(v\\) is aligned with the \\(y\\)-axis, then \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\lambda_2 x^2\\). (If \\(v\\) is any other vector then \\(x^{\\top}Ax\\) can be parameterized by \\(y = \\kappa x^2\\) for some \\(\\kappa\\) dependent upon \\(v\\).) The Rayleigh quotient at \\(v\\) is \\(\\lambda_{R}(v) = \\frac{\\lambda_1 x^2 }{ x^2} = \\lambda_1\\). The curvature of the parabola \\(y = \\lambda_1 x^2\\) at the origin is \\(2\\lambda_1\\). Thus the Rayleigh quotient is proportional to the the curvature of \\(x^{\\top}Ax\\) in the direction \\(v\\)!\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure5.png\" width=\"750\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 5:\u003C\u002Fb\u003E The quadratic form \\(x^{\\top}Ax\\) is shown in orange and \\(x^{\\top} x\\) is shown in blue. Intersecting both surfaces with a plane \\(h\\) gives two parabola. Within the plane \\(h\\) we can define a local coordinate system and parameterize both parabola as \\(\\kappa x^2\\) and \\(x^2\\). The Rayleigh quotient is equal to the ratio of the heights of the parabolas at any point, which is always equal to \\(\\kappa\\).\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EFrom this intuition it is clear that the value of the Rayleigh quotient is identical along any ray starting at, but not including, the origin. The length of \\(v\\) corresponds to the value of \\(x\\) in the coordinate system defined by \\(h\\), which does not affect the Rayleigh quotient. We can also see this algebraically, by choosing a unit vector \\(v\\) and parameterizing a ray in the direction \\(v\\) as \\(\\alpha v\\) for \\(\\alpha \\in \\mathbb{R}\\) and \\(\\alpha &gt; 0\\). Then we have that\u003C\u002Fp\u003E\n\u003Cspan class=\"math display\"\u003E\\[\\begin{aligned}\n\\lambda_{R}(\\alpha v) &amp;= \\frac{(\\alpha v^{\\top})A(\\alpha v)} {\\alpha^2 v^{\\top}v}\\\\\n                      &amp;= \\frac{v^{\\top}Av} {v^{\\top}v}\\\\\n                      &amp;= v^{\\top}Av.\n\\end{aligned}\\]\u003C\u002Fspan\u003E\n\u003Cp\u003EThus it is sufficient to consider the values of the Rayleigh quotient on the unit sphere.\u003C\u002Fp\u003E\nFor a unit vector \\(v\\) the value of the Rayleigh quotient can be written in the eigenbasis as\n\u003Cspan class=\"math display\"\u003E\\[\\begin{aligned}\nv^{\\top}Av = \\sum_{i=1}^{n} \\lambda_{i}\\langle v, u_i\\rangle^2\n\\end{aligned}\\]\u003C\u002Fspan\u003E\n\u003Cp\u003Ewhere \\(\\sum_{i=1}^{n} \\langle v, u_i\\rangle^2 = 1\\). Thus the Rayleigh quotient is a convex combination of the eigenvalues of \\(A\\) and so its value is bounded by the minimum and maximum eigenvalues \\( \\lambda_{n} \\leq \\lambda_{R}(v) \\leq \\lambda_{1}\\) for all \\(v\\). This fact is also easily seen from the geometric picture above, as the curvature at the origin is bounded by twice the minimum and maximum eigenvalues. It can be readily seen by either direct calculation or by the coefficients of the convex combination, that if \\(v\\) is an eigenvector, then \\(\\lambda_{R}(v)\\) is the corresponding eigenvalue of \\(v\\).\u003C\u002Fp\u003E\n\u003Cp\u003ERecall that a critical point of a function is a point where the derivative is equal to 0. It should come as no surprise that the eigenvalues are the critical values of the Rayleigh quotient and the eigenvectors are the critical points. What is less obvious is the special geometric structure of the critical points.\u003C\u002Fp\u003E\n\u003Cp\u003EThe gradient of the Rayleigh quotient is \\(\\frac{2}{v^{\\top}v}(Av - \\lambda_{R}(v)v)\\), from which it is easy to see that every eigenvector is a critical point of \\(\\lambda_{R}\\). The type of critical point is determined by the Hessian matrix, which at the critical point \\(u_i\\) is \\(2(A - \\lambda_{i}I)\\). The eigenvalues of the Hessian are \\(\\lambda_j - \\lambda_i\\) for \\(j \\in [1,n]\\). Assuming for a moment that the eigenvalues are all distinct, the matrix \\(2(A - \\lambda_{i}I)\\) has \\(i-1\\) eigenvectors that are positive, one eigenvalue that is 0, and \\(n - i\\) eigenvalues that are negative. The 0 eigenvalue represents the fact that the value of the Rayleigh quotient is unchanged along the ray \\(\\alpha u_i\\). The other eigenvalues represent the fact that at \\(u_i\\), along the unit sphere, there are \\(i-1\\) directions in which we can walk to increase the value of the Rayleigh quotient, and \\(n-i\\) directions that decrease the Rayleigh quotient. Thus each eigenvector gives rise to a different type of saddle, and there are exactly two critical points of each type on the unit sphere.\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure6.png\" style=\"width: 400px; margin:auto;\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 6:\u003C\u002Fb\u003E Contours of the Rayleigh quotient on the unit sphere and the gradient of the Rayleigh quotient at each point. We clearly see one minimum in blue corresponding to the minimum eigenvalue, one saddle point, and one maximum in bright yellow corresponding to the maximum eigenvalue.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EFinally we come to the crown jewel of the algorithms in this post. The Rayleigh Quotient Iteration algorithm simply updates the estimate \\(\\mu\\) at each iteration with the Rayleigh quotient. Other than this slight modification, the algorithm is exactly like Shifted Inverse iteration.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\ndef rayleigh_quotient_iteration(A, max_iter):\n  I = np.identity(A.shape[0])\n  v = np.random.randn(A.shape[0])\n  v \u002F= np.linalg.norm(v) #generate a uniformly random unit vector\n  mu = np.dot(v, np.dot(A, v))\n  for t in range(max_iter):\n    v = np.linalg.solve(mu * I - A, v) #compute (mu*I - A)^(-1)v\n    v \u002F= np.linalg.norm(v)\n    mu = np.dot(v, np.dot(A, v)) #compute Rayleigh quotient\n  return (v, mu)\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThis slight modification drastically improves the convergence rate. Unlike the other algorithms in this post which converge linearly, Rayleigh quotient iteration exhibits local cubic convergence! This means that, assuming \\(\\| v_{t} - u_i\\| \\leq \\epsilon\\) for some \\(u_i\\), on the next iteration we will have that \\(\\| v_{t+1} - u_{i} \\| \\leq \\epsilon^3\\). In practice this means that you should expect triple the number of correct digits at each iteration. It’s hard to understate how crazy fast cubic convergence is, and, to the best of the author’s knowledge, algorithms that exhibit cubic convergence are rare in the numerical algorithms literature.\u003C\u002Fp\u003E\n\u003Cfigure align=\"middle\"\u003E\n\u003Cimg src=\"numerical-algorithms-for-computing-eigenvectors\u002FFigure7v2.gif\" width=\"550\"\u003E\n\u003Cfigcaption\u003E\n\u003Cb\u003EFigure 7:\u003C\u002Fb\u003E The Rayleigh Quotient Iteration algorithm. After only 6 iterations the eigenvalue estimate \\(\\mu_t\\) is so accurate that the resulting matrix \\((\\mu_t I_{n} - A)\\) is singular up-to machine precision and we can no longer solve the system for an inverse. Note that every other figure in this post shows 50 iterations.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp\u003EIntuitively, the reason that Rayleigh Quotient Iteration exhibits cubic convergence is because, while the Shifted Inverse Iteration step converges linearly, the Rayleigh quotient is a quadratically good estimate of an eigenvalue near an eigenvector. To see this consider the Taylor series expansion of \\(\\lambda_{R}\\) near an eigenvector \\(u_i\\).\u003C\u002Fp\u003E\n\u003Cspan class=\"math display\"\u003E\\[\\begin{aligned}\n\\lambda_{R}(v) &amp;= \\lambda_{R}(u_i) + (v - u_{i})^{\\top} \\nabla \\lambda_{R}(u_i) + O(||v - u_i||^2)\\\\\n               &amp;= \\lambda_{R}(u_i) + O(||v - u_i||^2)\\\\\n\\lambda_{R}(v) - \\lambda_{R}(u_i) &amp;= O(||v - u_i||^2)                \n\\end{aligned}\\]\u003C\u002Fspan\u003E\n\u003Cp\u003EThe second step follows from the fact that \\(u_i\\) is a critical point of \\(\\lambda_{R}\\) and so \\(\\nabla \\lambda_{R}(u_i) = 0\\).\u003C\u002Fp\u003E\n\u003Cp\u003EWhile Rayleigh Quotient Iteration exhibits very fast convergence, it’s not without its drawbacks. First, notice that the system \\((\\mu_{t}I - A)^{-1}\\) changes at each iteration. Thus we cannot precompute a factorization of this matrix and quickly solve the system using forward and backward substitution at each iteration, like we did in the Shifted Inverse Iteration algorithm. We need to solve a different linear system at each iteration, which is much more expensive. Second, Rayleigh Quotient Iteration gives no control over to which eigenvector it converges. The eigenvector it converges to depends on which basin of attraction the initial random vector \\(v_{0}\\) falls into. Thus cubic convergence comes at a steep cost. This balance between an improved convergence rate and solving a different linear system at each iteration feels like mathematical poetic justice. The price to pay for cubic convergence is steep.\u003C\u002Fp\u003E\n"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.ab193722.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.ab193722.js")}document.head.appendChild(s)</script> <link href=client/main.3211539642.css rel=stylesheet><link href=client/[slug].c8947cd5.css rel=stylesheet><link href=client/client.ab193722.css rel=stylesheet> <title>Numerical Algorithms for Computing Eigenvectors</title><script data-svelte=svelte-puhaqb>var disqus_config = function () {
            let tokens = window.location.href.split('/');
            this.page.url = window.location.href;
            this.page.identifier = "/" + tokens[tokens.length-1];
        };
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://marckhoury-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })(); </script><noscript data-svelte=svelte-puhaqb>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript data-svelte=svelte-puhaqb>comments powered by Disqus.</a></noscript> <link href=/client/main.3211539642.css rel=preload as=style><link href=/client/[slug].c8947cd5.css rel=preload as=style><link href=/client/client.ab193722.css rel=preload as=style></head> <body> <div id=sapper> <nav class=svelte-4d1jzv><ul class=svelte-4d1jzv><li class="svelte-4d1jzv font-bold"><a href=. class=svelte-4d1jzv>home</a></li> <li class="svelte-4d1jzv font-bold"><a href=publications class=svelte-4d1jzv>publications</a></li> <li class="svelte-4d1jzv font-bold"><a href=hobbies class=svelte-4d1jzv>hobbies</a></li> <li class="svelte-4d1jzv font-bold"><a href=blog class=svelte-4d1jzv aria-current=page rel=prefetch>blog</a></ul> <ul class=svelte-4d1jzv><li class=svelte-4d1jzv><a href=https://twitter.com/marckkhoury class=svelte-4d1jzv><div class="svelte-4d1jzv icon twitter"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></div></a></li> <li class=svelte-4d1jzv><a href=https://github.com/marckhoury class=svelte-4d1jzv><div class="svelte-4d1jzv icon"><svg class=svelte-c8tyih viewBox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></div></a></li> <li class=svelte-4d1jzv><a href=https://www.linkedin.com/in/marckhoury/ class=svelte-4d1jzv><div class="svelte-4d1jzv icon linkedin"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></div></a></li> <li class=svelte-4d1jzv><a href=https://www.twitch.tv/thinkswithtwitch class=svelte-4d1jzv><div class="svelte-4d1jzv icon twitch"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M40.1 32L10 108.9v314.3h107V480h60.2l56.8-56.8h87l117-117V32H40.1zm357.8 254.1L331 353H224l-56.8 56.8V353H76.9V72.1h321v214zM331 149v116.9h-40.1V149H331zm-107 0v116.9h-40.1V149H224z"></path></svg></div></a></ul></nav> <main class=svelte-1uhnsl8> <h1>Numerical Algorithms for Computing Eigenvectors</h1> <div class="content svelte-gnxal1"><p>The eigenvalues and eigenvectors of a matrix are essential in many applications across the sciences. Despite their utility, students often leave their linear algebra courses with very little intuition for eigenvectors. In this post we describe several surprisingly simple algorithms for computing the eigenvalues and eigenvectors of a matrix, while attempting to convey as much geometric intuition as possible.</p> <p>Let \(A\) be a symmetric positive definite matrix. Since \(A\) is symmetric all of the eigenvalues of \(A\) are real and \(A\) has a full set of orthogonal eigenvectors. Let \(\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n > 0\) denote the eigenvalues of \(A\) and let \(u_{1}, \ldots, u_n\) denote their corresponding eigenvectors. The fact that \(A\) is positive definite means that \(\lambda_i > 0\) for all \(i\). This condition isn’t strictly necessary for the algorithms described below; I’m assuming it so that I can refer to the largest eigenvalue as opposed to the largest in magnitude eigenvalue.</p> <p>All of my intuition for positive definite matrices comes from the geometry of the quadratic form \(x^{\top}Ax\). Figure 1 plots \(x^{\top}Ax\) in \(\mathbb{R}^3\) for several \(2 \) matrices. When \(A\) is positive definite, the quadratic form \(x^{\top}Ax\) is shaped like a bowl. More rigorously it has positive curvature in every direction and the curvature at the origin in the direction of each eigenvector is proportional to the eigenvalue of that eigenvector. In \(\mathbb{R}^3\), the two eigenvectors give the directions of the maximum and minimum curvature at the origin. These are also known as principal directions in differential geometry, and the curvatures in these directions are known as principal curvatures. I often shorten this intuition by simply stating that positive definite matrices <em>are</em> bowls, because this is always the picture I have in my head when discussing them.</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure1v2.png width=750> <figcaption> <b>Figure 1:</b> The geometry of the quadratic form \(x^{\top}Ax\) for, from left to right, a positive definite matrix, a positive semi-definite matrix, an indefinite matrix, and a negative definite matrix. When \(A\) is positive definite it has positive curvature in every direction and is shaped like a bowl. The curvature at the origin in the direction of an eigenvector is proportional to the eigenvalue. A positive semi-definite matrix may have one or more eigenvalues equal to 0. This creates a flat (zero curvature) subspace of dimension equal to the number of eigenvalues with value equal to 0. An indefinite matrix has both positive and negative eigenvalues, and so has some directions with positive curvature and some with negative curvature, creating a saddle. A negative definite matrix has all negative eigenvalues and so the curvature in every direction is negative at every point. </figcaption> </figure> <p>Now suppose we wanted to compute a single eigenvector of \(A\). This problem comes up more often than you’d think and it’s a crime that undergraduate linear algebra courses don’t often make this clear. The first algorithm that one generally learns, and the only algorithm in this post that I knew as an undergraduate, is an incredibly simple algorithm called Power Iteration. Starting from a random unit vector \(v\) we simply compute \(A^{t}v\) iteratively. For sufficiently large \(t\), \(A^{t}v\) converges to the eigenvector corresponding to the largest eigenvalue of \(A\), hereafter referred to as the “top eigenvector”.</p> <pre><code class=language-python>
def power_iteration(A, max_iter):
  v = np.random.randn(A.shape[0])
  v /= np.linalg.norm(v) #generate a uniformly random unit vector
  for t in range(max_iter):
    v = np.dot(A, v) #compute Av
    v /= np.linalg.norm(v)
  return v
</code></pre> <p>To see why Power Iteration converges to the top eigenvector of \(A\) it helps to write \(v\) in the eigenbasis of \(A\) as \(v = \sum_{i=1}^n\beta_{i}u_{i}\) for some coefficients \(\beta_i\). Then we have that</p> <span class="display math">\[\begin{aligned} A^{t}v &= A^{t}(\sum_{i= 1}^{n}\beta_{i}u_{i})\\ &= \sum_{i=1}^{n}\beta_{i}A^{t}u_{i}\\ &= \sum_{i=1}^{n}\beta_{i}\lambda_{i}^{t}u_{i}\\ &= \lambda_{1}^t \sum_{i=1}^{n}\beta_{i}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^t u_{i}\\ &= \lambda_{1}^{t} \left( \beta_1 u_1 + \sum_{i=2}^{n}\beta_{i}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^t u_{i} \right). \end{aligned}\]</span> <p>Since \(\lambda_1\) is the largest eigenvalue, the fractions \(\left(\frac{\lambda_i}{\lambda_1}\right)^t\) go to 0 as \(t \rightarrow \infty\), for all \( i \neq 1\). Thus the only component of \(A^{t}v\) that has any weight is that of \(u_1\). How quickly each of those terms goes to 0 depends on the ratio \(\frac{\lambda_{2}}{\lambda_{1}}\). If this term is close to 1 then it may take many iterations to disambiguate between the top two (or more) eigenvectors. We say that the Power Iteration algorithm converges at a rate of \(O\left(\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^t\right)\), which for some unfortunate historical reason is referred to as “linear convergence”.</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure2.gif width=550> <figcaption> <b>Figure 2:</b> An illustration of the Power Iteration algorithm. The \(i\)th bar represents the component of the current iterate on the \(i\)th eigenvector, in order of decreasing eigenvalue. Notice that the components corresponding to the smallest eigenvalues decrease most rapidly, whereas the components on the largest eigenvalues take longer to converge. This animation represents 50 iterations of Power Iteration. </figcaption> </figure> <p>Power Iteration will give us an estimate of the top eigenvector \(u_1\), but what about the other extreme? What if instead we wanted to compute \(u_n\), the eigenvector corresponding to the smallest eigenvalue? It turns out there is a simple modification to the standard Power Iteration algorithm that computes \(u_n\). Instead of multiplying by \(A\) at each iteration, multiply by \(A^{-1}\). This works because the eigenvalues of \(A^{-1}\) are \(\frac{1}{\lambda_i}\), and thus the smallest eigenvalue of \(A\), \(\lambda_n\), corresponds to the largest eigenvalue of \(A^{-1}\), \(\frac{1}{\lambda_{n}}\). Furthermore the eigenvectors of \(A^{-1}\) are unchanged. This slight modification is called Inverse Iteration, and it exhibits the same convergence as Power Iteration, by the same analysis.</p> <pre><code class=language-python>
def inverse_iteration(A, max_iter):
  v = np.random.randn(A.shape[0])
  v /= np.linalg.norm(v) #generate a uniformly random unit vector
  lu, piv = scipy.linalg.lu_factor(A) # compute LU factorization of A
  for t in range(max_iter):
    v = scipy.linalg.lu_solve((lu, piv), v) #compute A^(-1)v
    v /= np.linalg.norm(v)
  return v
</code></pre> <p>Note that we don’t actually compute \(A^{-1}\) explicitly. Instead we compute an LU factorization of \(A\) and solve the system \(LUv_{t+1} = v_{t}\). The matrix that we’re multiplying by does not change at each iteration, so we can compute the LU factorization once and quickly solve a linear system to compute \(A^{-1}v\) at each iteration.</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure3.gif width=550> <figcaption> <b>Figure 3:</b> The Inverse Iteration algorithm. Notice that in this case the algorithm converges to the eigenvector corresponding to the smallest eigenvalue. </figcaption> </figure> <p>Power Iteration and Inverse Iteration find the eigenvectors at the extremes of the spectrum of \(A\), but sometimes we may want to compute a specific eigenvector corresponding to a specific eigenvalue. Suppose that we have an estimate \(\mu\) of an eigenvalue. We can find the eigenvector corresponding to the eigenvalue of \(A\) closest to \(\mu\) by a simple modification to Inverse Iteration. Instead of multiplying by \(A^{-1}\) at each iteration, multiply by \((\mu I_{n} - A)^{-1}\) where \(I_{n}\) is the identity matrix. The eigenvalues of \((\mu I_{n} - A)^{-1}\) are \(\frac{1}{\mu - \lambda_{i}}\). Thus the largest eigenvalue of \((\mu I_{n} - A)^{-1}\) corresponds to the eigenvalue of \(A\) whose value is closest to \(\mu\). By the same analysis as Power Iteration, Shifted Inverse Iteration also exhibits linear convergence. However the better the estimate \(\mu\) the larger \(\frac{1}{\mu - \lambda_{i}}\) and, consequently, the faster the convergence.</p> <pre><code class=language-python>
def shifted_inverse_iteration(A, mu, max_iter):
  I = np.identity(A.shape[0])
  v = np.random.randn(A.shape[0])
  v /= np.linalg.norm(v) #generate a uniformly random unit vector
  lu, piv = scipy.linalg.lu_factor(mu*I - A) # compute LU factorization of (mu*I - A)
  for t in range(max_iter):
    v = scipy.linalg.lu_solve((lu, piv), v) #compute (mu*I - A)^(-1)v
    v /= np.linalg.norm(v)
  return v
</code></pre> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure4.gif width=550> <figcaption> <b>Figure 4:</b> The Shifted Inverse Iteration algorithm. In this case we converge to the eigenvector corresponding to the eigenvalue nearest \(\mu\). </figcaption> </figure> <p>Shifted Inverse Iteration converges quickly if a good estimate of the target eigenvalue is available. However if \(\mu\) is a poor approximation of the desired eigenvalue, Shifted Inverse Iteration may take a long time to converge. In fact all of the algorithms we’ve presented so far have exactly the same convergence rate; they all converge linearly. If instead we could improve on the eigenvalue estimate at each iteration we could potentially develop an algorithm with a faster convergence rate. This is the main idea behind Rayleigh Quotient Iteration.</p> <p>The Rayleigh quotient is defined as \[ \lambda_{R}(v) = \frac{v^{\top} Av }{ v^{\top}v} \] for any vector \(v\). There are many different ways in which we can understand the Rayleigh quotient. Some intuition that is often given is that the Rayleigh quotient is the scalar value that behaves most like an “eigenvalue” for \(v\), even though \(v\) may not be an eigenvector. What is meant is that the Rayleigh quotient is the minimum to the optimization problem \[ \min_{\lambda \in \mathbb{R}} \|Av - \lambda v\|^2. \] This intuition is hardly satisfying.</p> <p>Let’s return to the geometry of the quadratic forms \(x^{\top}Ax\) and \(x^{\top}x\) which comprise the Rayleigh quotient, drawn in orange and blue respectively in Figure 5. Without loss of generality we can assume that \(A\) is a diagonal matrix. (This is without loss of generality because we’re merely rotating the surface so that the eigenvectors align with the \(x\) and \(y\) axes, which does not affect the geometry of the surface. This is a common trick in the numerical algorithms literature.) In this coordinate system, the quadratic form \(x^{\top}Ax = \lambda_1x_1^2 + \lambda_2 x_2^2\), where \(\lambda_1\) and \(\lambda_2\) are the diagonal entries, and thus the eigenvalues, of \(A\).</p> <p>Consider any vector \(v\) and let \(h = \operatorname{span}\{v, (0,0,1)\}\) be the plane spanned by \(v\) and the vector \((0,0,1)\). The intersection of \(h\) with the quadratic forms \(x^{\top}Ax\) and \(x^{\top}x\) is comprised of two parabolas, also shown in Figure 5. (This is a common trick in the geometric algorithms literature.) If \(v\) is aligned with the \(x\)-axis, then, within the coordinate system defined by \(h\), \(x^{\top}Ax\) can be parameterized by \(y = \lambda_1 x^2\) and \(x^{\top}x\) can be parameterized by \(y = x^2\). (Note that here \(y\) and \(x\) refer to local coordinates within \(h\) and are distinct from the vector \(x\) used in \(x^{\top}Ax\).) Similarly if \(v\) is aligned with the \(y\)-axis, then \(x^{\top}Ax\) can be parameterized by \(y = \lambda_2 x^2\). (If \(v\) is any other vector then \(x^{\top}Ax\) can be parameterized by \(y = \kappa x^2\) for some \(\kappa\) dependent upon \(v\).) The Rayleigh quotient at \(v\) is \(\lambda_{R}(v) = \frac{\lambda_1 x^2 }{ x^2} = \lambda_1\). The curvature of the parabola \(y = \lambda_1 x^2\) at the origin is \(2\lambda_1\). Thus the Rayleigh quotient is proportional to the the curvature of \(x^{\top}Ax\) in the direction \(v\)!</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure5.png width=750> <figcaption> <b>Figure 5:</b> The quadratic form \(x^{\top}Ax\) is shown in orange and \(x^{\top} x\) is shown in blue. Intersecting both surfaces with a plane \(h\) gives two parabola. Within the plane \(h\) we can define a local coordinate system and parameterize both parabola as \(\kappa x^2\) and \(x^2\). The Rayleigh quotient is equal to the ratio of the heights of the parabolas at any point, which is always equal to \(\kappa\). </figcaption> </figure> <p>From this intuition it is clear that the value of the Rayleigh quotient is identical along any ray starting at, but not including, the origin. The length of \(v\) corresponds to the value of \(x\) in the coordinate system defined by \(h\), which does not affect the Rayleigh quotient. We can also see this algebraically, by choosing a unit vector \(v\) and parameterizing a ray in the direction \(v\) as \(\alpha v\) for \(\alpha \in \mathbb{R}\) and \(\alpha > 0\). Then we have that</p> <span class="display math">\[\begin{aligned} \lambda_{R}(\alpha v) &= \frac{(\alpha v^{\top})A(\alpha v)} {\alpha^2 v^{\top}v}\\ &= \frac{v^{\top}Av} {v^{\top}v}\\ &= v^{\top}Av. \end{aligned}\]</span> <p>Thus it is sufficient to consider the values of the Rayleigh quotient on the unit sphere.</p> For a unit vector \(v\) the value of the Rayleigh quotient can be written in the eigenbasis as <span class="display math">\[\begin{aligned} v^{\top}Av = \sum_{i=1}^{n} \lambda_{i}\langle v, u_i\rangle^2 \end{aligned}\]</span> <p>where \(\sum_{i=1}^{n} \langle v, u_i\rangle^2 = 1\). Thus the Rayleigh quotient is a convex combination of the eigenvalues of \(A\) and so its value is bounded by the minimum and maximum eigenvalues \( \lambda_{n} \leq \lambda_{R}(v) \leq \lambda_{1}\) for all \(v\). This fact is also easily seen from the geometric picture above, as the curvature at the origin is bounded by twice the minimum and maximum eigenvalues. It can be readily seen by either direct calculation or by the coefficients of the convex combination, that if \(v\) is an eigenvector, then \(\lambda_{R}(v)\) is the corresponding eigenvalue of \(v\).</p> <p>Recall that a critical point of a function is a point where the derivative is equal to 0. It should come as no surprise that the eigenvalues are the critical values of the Rayleigh quotient and the eigenvectors are the critical points. What is less obvious is the special geometric structure of the critical points.</p> <p>The gradient of the Rayleigh quotient is \(\frac{2}{v^{\top}v}(Av - \lambda_{R}(v)v)\), from which it is easy to see that every eigenvector is a critical point of \(\lambda_{R}\). The type of critical point is determined by the Hessian matrix, which at the critical point \(u_i\) is \(2(A - \lambda_{i}I)\). The eigenvalues of the Hessian are \(\lambda_j - \lambda_i\) for \(j \in [1,n]\). Assuming for a moment that the eigenvalues are all distinct, the matrix \(2(A - \lambda_{i}I)\) has \(i-1\) eigenvectors that are positive, one eigenvalue that is 0, and \(n - i\) eigenvalues that are negative. The 0 eigenvalue represents the fact that the value of the Rayleigh quotient is unchanged along the ray \(\alpha u_i\). The other eigenvalues represent the fact that at \(u_i\), along the unit sphere, there are \(i-1\) directions in which we can walk to increase the value of the Rayleigh quotient, and \(n-i\) directions that decrease the Rayleigh quotient. Thus each eigenvector gives rise to a different type of saddle, and there are exactly two critical points of each type on the unit sphere.</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure6.png style=width:400px;margin:auto> <figcaption> <b>Figure 6:</b> Contours of the Rayleigh quotient on the unit sphere and the gradient of the Rayleigh quotient at each point. We clearly see one minimum in blue corresponding to the minimum eigenvalue, one saddle point, and one maximum in bright yellow corresponding to the maximum eigenvalue. </figcaption> </figure> <p>Finally we come to the crown jewel of the algorithms in this post. The Rayleigh Quotient Iteration algorithm simply updates the estimate \(\mu\) at each iteration with the Rayleigh quotient. Other than this slight modification, the algorithm is exactly like Shifted Inverse iteration.</p> <pre><code class=language-python>
def rayleigh_quotient_iteration(A, max_iter):
  I = np.identity(A.shape[0])
  v = np.random.randn(A.shape[0])
  v /= np.linalg.norm(v) #generate a uniformly random unit vector
  mu = np.dot(v, np.dot(A, v))
  for t in range(max_iter):
    v = np.linalg.solve(mu * I - A, v) #compute (mu*I - A)^(-1)v
    v /= np.linalg.norm(v)
    mu = np.dot(v, np.dot(A, v)) #compute Rayleigh quotient
  return (v, mu)
</code></pre> <p>This slight modification drastically improves the convergence rate. Unlike the other algorithms in this post which converge linearly, Rayleigh quotient iteration exhibits local cubic convergence! This means that, assuming \(\| v_{t} - u_i\| \leq \epsilon\) for some \(u_i\), on the next iteration we will have that \(\| v_{t+1} - u_{i} \| \leq \epsilon^3\). In practice this means that you should expect triple the number of correct digits at each iteration. It’s hard to understate how crazy fast cubic convergence is, and, to the best of the author’s knowledge, algorithms that exhibit cubic convergence are rare in the numerical algorithms literature.</p> <figure align=middle> <img src=numerical-algorithms-for-computing-eigenvectors/Figure7v2.gif width=550> <figcaption> <b>Figure 7:</b> The Rayleigh Quotient Iteration algorithm. After only 6 iterations the eigenvalue estimate \(\mu_t\) is so accurate that the resulting matrix \((\mu_t I_{n} - A)\) is singular up-to machine precision and we can no longer solve the system for an inverse. Note that every other figure in this post shows 50 iterations. </figcaption> </figure> <p>Intuitively, the reason that Rayleigh Quotient Iteration exhibits cubic convergence is because, while the Shifted Inverse Iteration step converges linearly, the Rayleigh quotient is a quadratically good estimate of an eigenvalue near an eigenvector. To see this consider the Taylor series expansion of \(\lambda_{R}\) near an eigenvector \(u_i\).</p> <span class="display math">\[\begin{aligned} \lambda_{R}(v) &= \lambda_{R}(u_i) + (v - u_{i})^{\top} \nabla \lambda_{R}(u_i) + O(||v - u_i||^2)\\ &= \lambda_{R}(u_i) + O(||v - u_i||^2)\\ \lambda_{R}(v) - \lambda_{R}(u_i) &= O(||v - u_i||^2) \end{aligned}\]</span> <p>The second step follows from the fact that \(u_i\) is a critical point of \(\lambda_{R}\) and so \(\nabla \lambda_{R}(u_i) = 0\).</p> <p>While Rayleigh Quotient Iteration exhibits very fast convergence, it’s not without its drawbacks. First, notice that the system \((\mu_{t}I - A)^{-1}\) changes at each iteration. Thus we cannot precompute a factorization of this matrix and quickly solve the system using forward and backward substitution at each iteration, like we did in the Shifted Inverse Iteration algorithm. We need to solve a different linear system at each iteration, which is much more expensive. Second, Rayleigh Quotient Iteration gives no control over to which eigenvector it converges. The eigenvector it converges to depends on which basin of attraction the initial random vector \(v_{0}\) falls into. Thus cubic convergence comes at a steep cost. This balance between an improved convergence rate and solving a different linear system at each iteration feels like mathematical poetic justice. The price to pay for cubic convergence is steep.</p> <section id=disqus_thread></section></div></main></div> 